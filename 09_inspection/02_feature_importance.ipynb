{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will discuss the feature importance provided by scikit-learn and the potential caveat. Tree-based model provide a fitted attribute named `feature_importances_` based on the mean decrease in impurity (MDI)\n",
    "\n",
    "We will show that you need to be careful whenever using it and provide an alternative.\n",
    "\n",
    "We will illustrate our example on the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better illustrate the limitation of the MDI approach, we will add some random features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(seed=42)\n",
    "X['random_cat'] = rng.randint(3, size=X.shape[0])\n",
    "X['random_num'] = rng.randn(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['pclass', 'sex', 'embarked', 'random_cat']\n",
    "numerical_columns = ['age', 'sibsp', 'parch', 'fare', 'random_num']\n",
    "\n",
    "X = X[categorical_columns + numerical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify some explanation, we will make a single train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our predictive pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will use a `RandomForestClassifier`, we will encode our categorical columns using an `OrdinalEncoder`. Let's first create our preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "categorical_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "numerical_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    [('cat', categorical_encoder, categorical_columns),\n",
    "     ('num', numerical_pipe, numerical_columns)])\n",
    "preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create our full model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = Pipeline([\n",
    "    ('preprocess', preprocessing),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of the Model\n",
    "Prior to inspecting the feature importances, it is important to check that\n",
    "the model predictive performance is high enough. Indeed there would be little\n",
    "interest of inspecting the important features of a non-predictive model.\n",
    "\n",
    "Here one can observe that the train accuracy is very high (the forest model\n",
    "has enough capacity to completely memorize the training set) but it can still\n",
    "generalize well enough to the test set thanks to the built-in bagging of\n",
    "random forests.\n",
    "\n",
    "It might be possible to trade some accuracy on the training set for a\n",
    "slightly better accuracy on the test set by limiting the capacity of the\n",
    "trees (for instance by setting ``min_samples_leaf=5`` or\n",
    "``min_samples_leaf=10``) so as to limit overfitting while not introducing too\n",
    "much underfitting.\n",
    "\n",
    "However let's keep our high capacity random forest model for now so as to\n",
    "illustrate some pitfalls with feature importance on variables with many\n",
    "unique values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"RF train accuracy: {model.score(X_train, y_train):.3f}\")\n",
    "print(f\"RF test accuracy: {model.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree's Feature Importance from Mean Decrease in Impurity (MDI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    Plot the values reported by `feature_importances_`. Is there anything looking strange?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_19.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/solution_20.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The impurity-based feature importance ranks the numerical features to be the\n",
    "most important features. As a result, the non-predictive ``random_num``\n",
    "variable is ranked the most important!\n",
    "\n",
    "This problem stems from two limitations of impurity-based feature\n",
    "importances:\n",
    "\n",
    "- impurity-based importances are biased towards high cardinality features;\n",
    "- impurity-based importances are computed on training set statistics and\n",
    "  therefore do not reflect the ability of feature to be useful to make\n",
    "  predictions that generalize to the test set (when the model has enough\n",
    "  capacity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative, the permutation importances of ``model`` are computed on a\n",
    "held out test set. This shows that the low cardinality categorical feature,\n",
    "``sex`` is the most important feature.\n",
    "\n",
    "Also note that both random features have very low importances (close to 0) as\n",
    "expected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(\n",
    "    model, X_test, y_test, n_repeats=10,\n",
    "    random_state=42, n_jobs=2\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "importances = pd.DataFrame(\n",
    "    result.importances[sorted_idx].T,\n",
    "    columns=X_test.columns[sorted_idx],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.boxplot(data=importances, orient=\"h\", color=\"tab:blue\", ax=ax)\n",
    "_ = ax.set_title(\"Permutation Importances (test set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to compute the permutation importances on the training\n",
    "set. This reveals that ``random_num`` gets a significantly higher importance\n",
    "ranking than when computed on the test set. The difference between those two\n",
    "plots is a confirmation that the RF model has enough capacity to use that\n",
    "random numerical feature to overfit. You can further confirm this by\n",
    "re-running this example with constrained random forest with `min_samples_leaf=10`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(\n",
    "    model, X_train, y_train, n_repeats=10,\n",
    "    random_state=42, n_jobs=2\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "importances = pd.DataFrame(\n",
    "    result.importances[sorted_idx].T,\n",
    "    columns=X_test.columns[sorted_idx],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.boxplot(data=importances, orient=\"h\", color=\"tab:blue\", ax=ax)\n",
    "_ = ax.set_title(\"Permutation Importances (train set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    Create a linear model to deal with the dataset.\n",
    "    <ul>\n",
    "        <li>Compute the permutation feature importances.</li>\n",
    "        <li>Compare it with the coefficients provided by the model.</li>\n",
    "    </ul>\n",
    "    Make those analysis on the train set.\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_21.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_22.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%# %load solutions/solution_23.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_24.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_25.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_26.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_27.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
