{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0709fa4-db18-4310-9d21-3666fd78583f",
   "metadata": {},
   "source": [
    "# Linear models - Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e327e94-88fe-44f1-a11d-cac727ebf1f9",
   "metadata": {},
   "source": [
    "In this notebook, we will discuss the importance of preprocessing in linear models, especially when the solver used rely on gradient-based optimization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355d294d-691d-4035-bca3-c088d87fa0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary fix to avoid spurious warning raised in scikit-learn 1.0.0\n",
    "# it will be solved in scikit-learn 1.0.1\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X has feature names.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1c9540-ddb1-48b1-a81a-c20b811b74f6",
   "metadata": {},
   "source": [
    "## Importance of data scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38881c73-d8b0-457d-a737-79089791f6d3",
   "metadata": {},
   "source": [
    "Since we would like to demonstrate issue related to gradient-based optimization solver, we will need to use a linear model that does not rely on an algorithm that provide a closed-form solution. Such algorithm is `LogisticRegression` for instance (in the contrary to `LinearRegression` or `Ridge`).\n",
    "\n",
    "Thus, let's start by loading our penguins dataset to distinguish the different species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a595eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../datasets/penguins_classification.csv\")\n",
    "target_name = \"Species\"\n",
    "X = data.drop(columns=target_name)\n",
    "y = data[target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5826886d-8733-4cc4-8443-e16d0c6fa440",
   "metadata": {},
   "source": [
    "Up to now, we did not bother much about evaluating our model: we used a single dataset just to illustrate some fitting property of the different estimators. However, the preprocessing that we are going to use need to be applied in a specific manner depending if one is training or testing a model. Therefore, we will start by using a training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2c7241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0\n",
    ")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e4d50e-10bc-401c-bf5c-40c1ed6a1b6f",
   "metadata": {},
   "source": [
    "Previously, we showed that we could train a `LogisticRegression` model in the following manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a62475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89018b19-6bbf-4821-b11b-d37e8824ba32",
   "metadata": {},
   "source": [
    "We previously stated that this model is minimizing a specific loss function (log loss). However, we did not mentioned what algorithm was used to find the optimal parameters $\\beta$ that minimize this log loss. We only discuss such details with `LinearRegression` where we used the Normal equation that is a closed-form solution to the least squared minimization.\n",
    "\n",
    "For the `LogisticRegression`, the problem does not have a closed-form solution. Instead, the different algorithms rely on the derivatives of the log loss to find the best parameter. One can check the available algorithm looking at the documentation of the `solver` parameter: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "All the different solvers rely on derivatives of the log loss, meaning that an iterative algorithm will take place to find the optimal parameters of the model. Therefore, once the `LogisticRegression` is fitted, we can know the number of iteration that an algorithm did to find the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297449a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f0bbf4-b994-478a-9cb3-d45c8a301af8",
   "metadata": {},
   "source": [
    "However, there is something that is not really proper regarding our training dataset. We can have a look to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ada9ce-d1e9-43c3-9265-c35837c8ba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "_, ax = plt.subplots(figsize=(8, 8))\n",
    "ax = sns.kdeplot(\n",
    "    data=X_train,\n",
    "    x=\"Culmen Length (mm)\",\n",
    "    y=\"Culmen Depth (mm)\",\n",
    "    levels=10,\n",
    "    fill=True,\n",
    "    cmap=plt.cm.viridis,\n",
    "    ax=ax,\n",
    ")\n",
    "_ = ax.axis(\"square\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192947ba-a636-453c-8b54-2eabbdfaad0c",
   "metadata": {},
   "source": [
    "Looking at our data distribution, we observed that the deviation from the mean is more important for the culmen length feature than the culmen depth feature. This will have an impact when dealing with gradient-based model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f083ad5-4845-4ffd-ab9c-20578937ae38",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    <ul>\n",
    "        <li>Using pandas, rescale the dataset such that they will be centered to (0, 0) and have a unit standard deviation for both feature.</li>\n",
    "        <li>Plot the distribution of the data as previously done.</li>\n",
    "        <li>Fit a <tt>LogisticRegression</tt> on the scaled dataset.</li>\n",
    "        <li>Check the number of iterations needed to train the model.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa6d11-e6bf-47f0-83ec-c44ef02f0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_25.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790992aa-f207-4b3c-b63d-54c3f31acafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_26.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f923e-ae55-45f3-ab44-1c744ca9b84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_27.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1caa206-d7e2-4302-b5be-17ece84b9749",
   "metadata": {},
   "source": [
    "## Scikit-learn transformers API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd77b08-0601-4199-9b2c-46c78e9ab625",
   "metadata": {},
   "source": [
    "There is a family of estimator in scikit-learn that allows to \"transform\" data. As predictor, they can learn some states during `fit` and later reuse these states when calling the method `transform`. Let's perform the previous scaling using the `StandardScaler` transformer from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58571235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b2d62-0a5a-420d-b612-dcaf2f6f40f6",
   "metadata": {},
   "source": [
    "As stated, we expect our scaler to have some state after calling `fit`. Indeed, our scaler will have store the mean and standard deviation of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb7b3c-2525-4726-a137-bc3a30b306f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.mean_, scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c845a85-6113-4096-aa7b-eff57087307a",
   "metadata": {},
   "source": [
    "Now we can use the `transform` method to scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a41cf3-db0a-4368-be13-79065dce8832",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.transform(X_train)\n",
    "# scikit-learn will transform any pandas dataframe into a NumPy array\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b430f198-a741-4976-bfc3-1cc665c15a38",
   "metadata": {},
   "source": [
    "Let's plot the distribution of the data to convince ourself that the transformation applied is the same than we did by hand earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c2c32-d807-40d1-9655-b18b7a2bb390",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(8, 8))\n",
    "ax = sns.kdeplot(\n",
    "    data=X_train_scaled,\n",
    "    x=\"Culmen Length (mm)\",\n",
    "    y=\"Culmen Depth (mm)\",\n",
    "    levels=10,\n",
    "    fill=True,\n",
    "    cmap=plt.cm.viridis,\n",
    "    ax=ax,\n",
    ")\n",
    "_ = ax.axis(\"square\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80247c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4539ac7b-17ea-4bff-86fa-43d4ee6a4d27",
   "metadata": {},
   "source": [
    "The advantage of using scikit-learn transformer over manually manipulating the dataset is that we can make complex pipeline. A pipeline can be represented as a sequence of scikit-learn transformers finishing by a scikit-learn predictor. This pipeline will have the same API than a scikit-learn predictor (i.e. `fit`, `predict`, `predict_proba`, `decision_function`), and will take care of the transformation and the transformation state for us.\n",
    "\n",
    "Let's define such of a scikit-learn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb42b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "# to make nice diagram when plotting complex pipeline\n",
    "sklearn.set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb86ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842c799-036e-4c0a-a998-41186ae42e47",
   "metadata": {},
   "source": [
    "Here, we created a pipeline that will be in charge of scaling the data first and then pass it to the classifier. Let's demonstrate how to train this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda70405-5975-4712-8eed-86eeb28a565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398d0fd2-dff3-4e01-81ce-db715db26602",
   "metadata": {},
   "source": [
    "We just trained our model without the need to take care about the scaling ourselve. We can check that model have internal state learnt during `fit`. Let's first check our scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fe37b9-b61f-4ccd-bd25-a8ef3266610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[0].mean_, model[0].scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fbfb5c-a2b0-419b-82f3-5dc51641dd5a",
   "metadata": {},
   "source": [
    "So the first step of the pipeline stored the mean and standard deviation of the training set. Did we learn the optimal parameter of the `LogisticRegression`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7390bbde-6963-4f86-8543-e3d1eab6599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[-1].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8341df0a-aee7-4ef3-9c36-6098624a2734",
   "metadata": {},
   "source": [
    "Apparently, we did. We can even check the number of iteration that it took to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f533d4b-3068-46db-8058-2a5e10151f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[-1].n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cfc328-9620-452c-a78b-cd6b848830b7",
   "metadata": {},
   "source": [
    "So the pipeline did all the job that we previously manually did with the advantage that it exposes the same API than any predictor that we used up to now. What about prediction and scoring?\n",
    "\n",
    "Indeed, during prediction, we would need to scale the testing set using the statistic found during training. Using the pipeline with the `predict` method will take care of this processing for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fcabbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4506576-343b-4b59-ae84-0d7d0dc26cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_test == y_pred).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ebf65-4036-4c7a-9ef7-8447666d4ce2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    In the first course, we saw that we could pass a model to <tt>cross_validate</tt> to get a distrbution of score.\n",
    "    Use the previous complex pipeline, and evaluate it using the <tt>cross_validate</tt> function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5285af0-7c80-495d-b8d4-6aa8e964372a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcfadaf2-f73f-44cb-9038-41b677b32175",
   "metadata": {},
   "source": [
    "## Side effect of longer processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd75f8b-6a1d-49d0-857c-6bd63d48b6bb",
   "metadata": {},
   "source": [
    "We would like to trigger a behaviour that you could encounter in scikit-learn. It is quite important to know what it means and how to act. Let's load a dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a2820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/adult-census-numeric-all.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcaecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"class\"\n",
    "X = data.drop(columns=target_name)\n",
    "y = data[target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82020403-826f-4384-a802-74083e3e464b",
   "metadata": {},
   "source": [
    "This dataset is linked to a classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5b5501-c50d-4190-94d1-015f951e29a1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    Fit a <tt>LogisticRegression</tt> algorithm without scaling the data first. In addition, force the maximum number of iteration of the solver to be at most 50 iterations. What is the result of the training?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e599906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_28.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939c309f-a26e-46ab-b65d-1f993573f34f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    Implement both proposals stipulated in the warning message and argument which option you should choose and why?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ef774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_29.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3638f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_30.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dda2212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_31.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948a4232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_32.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a343f2cc-dbe1-4de1-b335-f0172c6a3790",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    What is the impact of using a scaler on the coefficient?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeee109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_33.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_34.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
