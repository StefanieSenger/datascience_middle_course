{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4472c949-f041-423d-94d3-4d876acf489f",
   "metadata": {},
   "source": [
    "# Linear models - Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0580397f-e9f0-4a98-b3b6-f585c46fa4ae",
   "metadata": {},
   "source": [
    "In this notebook, we will focus on the concept of regularization in linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab93ac2-30fc-4cc7-9dbf-f94fd46e7f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary fix to avoid spurious warning raised in scikit-learn 1.0.0\n",
    "# it will be solved in scikit-learn 1.0.1\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X has feature names.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5b681a-6c41-4d5b-9607-331c096c5209",
   "metadata": {},
   "source": [
    "## Introductionary example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9aa4c-4c0a-4696-9a76-cb2fb91bfa93",
   "metadata": {},
   "source": [
    "In this first example, we will show a known issue due to correlated features when fitting a linear model.\n",
    "\n",
    "The data generative process to create the data is a linear relationship between the features and the target. However, out of 5 features, only 2 features will be used while 3 other features will not be linked to the target. In addition, a little bit of noise will be added. When generating the dataset, we can as well get the true model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2c2850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "data, target, coef = make_regression(\n",
    "    n_samples=2_000,\n",
    "    n_features=5,\n",
    "    n_informative=2,\n",
    "    shuffle=False,\n",
    "    coef=True,\n",
    "    random_state=0,\n",
    "    noise=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80293a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "feature_names = [f\"Features {i}\" for i in range(data.shape[1])]\n",
    "coef = pd.Series(coef, index=feature_names)\n",
    "coef.plot.barh()\n",
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ae0dba-79c4-4311-89b2-a40ad11f2220",
   "metadata": {},
   "source": [
    "Plotting the true coefficients, we observe that only 2 features out of the 5 features as an impact on the target.\n",
    "\n",
    "Now, we will fit a linear model on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b4d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(data, target)\n",
    "linear_regression.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d492a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [f\"Features {i}\" for i in range(data.shape[1])]\n",
    "coef = pd.Series(linear_regression.coef_, index=feature_names)\n",
    "_ = coef.plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96550cc6-2aac-4d06-9446-917ac59233fd",
   "metadata": {},
   "source": [
    "We observe that we can recover almost the true coefficients. The small fluctuation are due to the noise that we added into the dataset when generating it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc11946b-a410-4f6b-b6e5-3bf828c6c276",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    Now, we will create some correlated feature and observe what is the impact.\n",
    "    <ul>\n",
    "        <li>Using <tt>X</tt>, add 4 new features that will be a repetition of features #0 and #1.</li>\n",
    "        <li>Fit a <tt>LinearRegression</tt> model.</li>\n",
    "        <li>Plot the value of the coefficients.</li>\n",
    "    </ul>\n",
    "    Are the coefficient values meaningful in regarde to the true coefficients used to generate the dataset?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2798703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_10.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f3f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_11.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db3380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_12.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0441d-5b39-4193-b994-b486a72a23e0",
   "metadata": {},
   "source": [
    "## Ridge regressor - L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cbe1af-63b8-4d45-8950-94b3f4a2b502",
   "metadata": {},
   "source": [
    "We saw that our linear model does not succeed to recover the true coefficients. This is due to the fact that the coefficients are not constraints and that the problem is ill-posed due to the multicollinearity. We can solve this issue by imposing some constraints on the weights of the model: this is call regularization. One possible solution is to imposed an L2 regularization on the weights. The loss to be minimized becomes:\n",
    "\n",
    "$$\n",
    "loss = (y - X \\beta)^2 + \\alpha \\|\\beta\\|_2\n",
    "$$\n",
    "\n",
    "This regularization will enforce the weights to shrink towards zero. The parameter controlling the shrinking is the parameter $\\alpha$.\n",
    "Such a regression model is known as `Ridge` in scikit-learn. Let's fit such model and check the effect on the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c716fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=1)\n",
    "ridge.fit(data, target)\n",
    "ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a49b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.Series(ridge.coef_, index=feature_names)\n",
    "_ = coef.plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5136deb7-6099-4148-b482-895c55db7fa1",
   "metadata": {},
   "source": [
    "Applying a small regularization solves the problem of the weight. We can even find the original relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.coef_[:5] * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4694937-8657-43a2-ac7d-a3994844c1ac",
   "metadata": {},
   "source": [
    "By looking at the loss, we see that increasing $\\alpha$ will shrink more the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff6e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=1e5)\n",
    "ridge.fit(data, target)\n",
    "ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.Series(ridge.coef_, index=feature_names)\n",
    "_ = coef.plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e536f1-55a3-4876-98fc-b40a2beb0155",
   "metadata": {},
   "source": [
    "We see that the weights shrinked toward zeros but the relative weights of each feature remained the same. We can now verify that passing a very small $\\alpha$ will be equivalent to using a `LinearRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69428fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=1e-14)\n",
    "ridge.fit(data, target)\n",
    "ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d0f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.Series(ridge.coef_, index=feature_names)\n",
    "_ = coef.plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c075dde-7281-45fc-bcd2-355dee677dcf",
   "metadata": {},
   "source": [
    "## Lasso regressor - L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2313bc-7e43-4a2f-95db-e256f3972b8d",
   "metadata": {},
   "source": [
    "Another possible type of regularization is L1. It can be formalized with:\n",
    "\n",
    "$$\n",
    "loss = (y - X \\beta)^2 + \\alpha \\|\\beta\\|_1\n",
    "$$\n",
    "\n",
    "This type of regressor in called `Lasso` in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8311ed39-df10-4424-b581-28515031c3f0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    Repeat the previous experiment varying the value of $\\alpha$ and check what is the impact of $\\alpha$ on the weights $\\beta$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4124c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_13.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb841a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_14.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e41ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_15.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5753790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_16.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0613b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_17.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a5e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_18.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ab81d-7efe-4919-81ef-9d0ffe569ee6",
   "metadata": {},
   "source": [
    "## Elastic net - Combining both L2 and L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87372f2c-ac66-4ee7-a2a8-d89c216d4ec2",
   "metadata": {},
   "source": [
    "Sometimes we can be interested by combining both L2 and L1 regularization: find a subset of features that should contribute to the target prediction but, at the same time, avoid that the non-null coefficients to be arbritary large without any constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d417a80-c778-4c3f-8229-1eae684bb917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "elastic_net.fit(data, target)\n",
    "elastic_net.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d2f82-4109-4ee3-a95a-04fd01896ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.Series(elastic_net.coef_, index=feature_names)\n",
    "_ = coef.plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9ab0db-b47f-4695-b39c-3db040b4abe5",
   "metadata": {},
   "source": [
    "## What about classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44b34e-cd88-4d25-aa9d-aed918672a45",
   "metadata": {},
   "source": [
    "In classification, the different choices of regularization do not give rise to different estimator. Instead, the regularization can be set as a parameter of the linear model itself. In classification, there is mainly two models: `LogisticRegression` and `LinearSVC`. They vary in the fact that they minimized different losses. However, they both have a parameter `penalty` and a parameter `C` (that is the inverse of `alpha` in regression).\n",
    "\n",
    "In this section, we will use a `LogisticRegression` and we will check the impact of the parameter `C`. Let's first load a classification dataset. Here, we want to predict the penguin species from two features that are the culmen length and depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebed694",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/penguins_classification.csv\")\n",
    "data = data[data[\"Species\"].isin([\"Adelie\", \"Chinstrap\"])]\n",
    "data[\"Species\"] = data[\"Species\"].astype(\"category\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bdae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]], data[\"Species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36294664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = data.plot.scatter(\n",
    "    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", c=\"Species\",\n",
    "    cmap=plt.cm.RdBu, s=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0806c7f-0f4d-429c-acb1-5d5c6cef49e0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>QUESTION</b>:</p>\n",
    "    Looking at the documentation, what is the default regularization (or no regularization) by default in <tt>LogisticRegression</tt>?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576de070-f8b1-43b7-8c64-419937beb6aa",
   "metadata": {},
   "source": [
    "Below, we fit a `LogisticRegression` model with the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf567c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15167ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.plotting import DecisionBoundaryDisplay\n",
    "\n",
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    model,\n",
    "    X,\n",
    "    response_method=\"decision_function\",\n",
    "    cmap=plt.cm.RdBu,\n",
    "    plot_method=\"pcolormesh\",\n",
    "    shading=\"auto\",\n",
    ")\n",
    "_ = data.plot.scatter(\n",
    "    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", c=\"Species\",\n",
    "    cmap=plt.cm.RdBu, s=50, ax=display.ax_\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a642d11b-1b4d-4be3-9242-c8fb77e5645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.Series(model.coef_[0], index=X.columns)\n",
    "_ = coef.plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ea4408-0fc2-4683-9776-1f449cd12ba1",
   "metadata": {},
   "source": [
    "We can use this example as a baseline to observe the effect of the parameter `C`. In addition, we can provide that the loss function of a logistic regression is the following:\n",
    "\n",
    "$$\n",
    "loss = \\frac{1 - \\rho}{2} w^T w + \\rho \\|w\\|_1 + C \\log ( \\exp (y_i (X \\beta)) + 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d338482-b0e0-45c7-a354-853e35d1a8e2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    Repeat a fit and check the impact of the paramter <tt>C</tt> on both the coefficients and the decision boundary of the classifier.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78724061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_19.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e46983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_20.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d980ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_21.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1e1856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_22.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70662fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_23.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b082ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_24.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a0d9a-f7d4-4385-8117-e733a555e02a",
   "metadata": {},
   "source": [
    "Looking at the loss above, we see that the parameter `C` is applied on the data term (computing the error between the true and predicted target). In regression, this regularization weight $\\alpha$ was applied on the weights instead. For this reason, `C` have the inverse impact than $\\alpha$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
