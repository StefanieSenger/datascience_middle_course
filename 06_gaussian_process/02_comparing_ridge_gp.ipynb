{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247e7461-1c30-4e56-94c5-a3e736ee2824",
   "metadata": {},
   "source": [
    "# Comparing Kernel Ridge and Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c76752c-cf3a-4e28-8d32-e1128d5a6b94",
   "metadata": {},
   "source": [
    "In this notebook, we will make some small exercises to check the difference between some ridge regressors and gaussian process models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa10dc6-ae00-4f14-891e-f4b4d86c7d9d",
   "metadata": {},
   "source": [
    "Firs, we create a synthetic dataset. The true generative process will take a 1-D\n",
    "vector and compute its sine. Note that the period of this sine is thus\n",
    "$2 \\pi$. We will reuse this information later in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e574ef72-1ce3-45c5-bb1c-33832bee6513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "data = np.linspace(0, 30, num=1_000).reshape(-1, 1)\n",
    "target = np.sin(data).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c970e7ac-e298-4c0d-a5aa-dbb0f780b0b2",
   "metadata": {},
   "source": [
    "Now, we can imagine a scenario where we get observations from this true\n",
    "process. However, we will add some challenges:\n",
    "\n",
    "- the measurements will be noisy;\n",
    "- only samples from the beginning of the signal will be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b0ee2b-717e-4d2c-865a-b739561797bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sample_indices = rng.choice(np.arange(0, 400), size=40, replace=False)\n",
    "training_data = data[training_sample_indices]\n",
    "training_noisy_target = target[training_sample_indices] + 0.5 * rng.randn(\n",
    "    len(training_sample_indices)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d885577-432c-45dc-85dd-55312c645ba2",
   "metadata": {},
   "source": [
    "Let's plot the true signal and the noisy measurements available for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a1587-0abb-470b-a42b-827c8649ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "sklearn.set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9083201-93c7-48bd-ae1f-187596b0492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(data, target, label=\"True signal\", linewidth=2)\n",
    "plt.scatter(\n",
    "    training_data,\n",
    "    training_noisy_target,\n",
    "    color=\"black\",\n",
    "    label=\"Noisy measurements\",\n",
    ")\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "_ = plt.title(\n",
    "    \"Illustration of the true generative process and \\n\"\n",
    "    \"noisy measurements available during training\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd0ad05-39c6-4adb-9004-a23f175f5dbe",
   "metadata": {},
   "source": [
    "## Limitations of a simple linear model\n",
    "\n",
    "First, we would like to highlight the limitations of a linear model given\n",
    "our dataset. We fit a `Ridge` and check the\n",
    "predictions of this model on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9865b015-e47c-48b5-9ec7-41ebe3dd780a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    <ul>\n",
    "        <li>Fit a <tt>Ridge</tt> model on the training samples.</li>\n",
    "        <li>Predict with the trained model for all data point.</li>\n",
    "    </ul>\n",
    "    What is the limitation of the <tt>Ridge</tt> model?\n",
    "    <br>\n",
    "    How could you bypass these limitation?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ef5f32-24a9-44e7-a682-34e0ddc4c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7acfb83-f39c-4b2f-889d-55f9bf974bb9",
   "metadata": {},
   "source": [
    "Such a ridge regressor underfits data since it is not expressive enough.\n",
    "\n",
    "## Kernel methods: kernel ridge and Gaussian process\n",
    "\n",
    "### Kernel ridge\n",
    "\n",
    "We previously saw that we could make a linear model more expressive notably by using kernel approximation. Here, we will even use the \"kernel trick\" in conjuction with `Ridge`. Indeed, scikit-learn propose the `KernelRidge` regressor that accept any type of kernel, similarly to SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2dc3d3-b888-427f-b0b8-eadf236f3289",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    <ul>\n",
    "        <li>Fit a <tt>KernelRidge</tt> using a <tt>ExpSineSquared</tt> kernel, all with default parameters.</li>\n",
    "        <li>Measure the time to fit the model.</li>\n",
    "        <li>Plot the predictions for the entire dataset.</li>\n",
    "    </ul>\n",
    "    Is there any issue with the model? How to solve them, is there is any?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14632516-3097-494f-85b8-b28f57e776f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_02.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00868680-0ccd-4b0e-b7f2-d50a767c66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_03.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d53b67e-9ee3-46ee-b9c2-a53dda02478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_04.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f164b814-63ba-4b77-abd2-2ec5b3ff8081",
   "metadata": {},
   "source": [
    "This fitted model is not accurate. Indeed, we did not set the parameters of\n",
    "the kernel and instead used the default ones. We can inspect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9ca43-7dd1-4c5e-b5b3-658d361fb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_ridge.kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21031f5d-3bf4-4ee1-986d-67d9d574aadf",
   "metadata": {},
   "source": [
    "Our kernel has two parameters: the length-scale and the periodicity. For our\n",
    "dataset, we use `sin` as the generative process, implying a\n",
    "$2 \\pi$-periodicity for the signal. The default value of the parameter\n",
    "being $1$, it explains the high frequency observed in the predictions of\n",
    "our model.\n",
    "Similar conclusions could be drawn with the length-scale parameter. Thus, it\n",
    "tell us that the kernel parameters need to be tuned. We will use a randomized\n",
    "search to tune the different parameters the kernel ridge model: the `alpha`\n",
    "parameter and the kernel parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf34cfb-8cb3-42ef-9cdc-184d398f4355",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    Using the previous information\n",
    "    <ul>\n",
    "        <li>Create a <tt>RandomizedSearchCV</tt> and tune the parameter of the kernel and the regularization parameters.</li>\n",
    "        <li>Measure the time to fit the model.</li>\n",
    "        <li>Plot the predictions for the entire dataset.</li>\n",
    "    </ul>\n",
    "    Is the model properly fitting the data? What is the computational cost of the model?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd078c-9b33-4f52-bc2c-91412b5f723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_05.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342e27dc-d5f8-42ea-ba21-6f0c120afc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_06.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73d45f1-3da8-47ce-81cc-60626cf35812",
   "metadata": {},
   "source": [
    "Fitting the model is now more computationally expensive since we have to try\n",
    "several combinations of hyperparameters. We can have a look at the\n",
    "hyperparameters found to get some intuitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb82c8-c1eb-4c2e-93eb-9d373ec3a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_07.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d5e88-0c19-441e-b151-dbea85c8e9ec",
   "metadata": {},
   "source": [
    "Looking at the best parameters, we see that they are different from the\n",
    "defaults. We also see that the periodicity is closer to the expected value $2 \\pi$. We can now inspect the predictions of our tuned kernel ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5871a1f9-6eb3-45f2-acff-a12b11583524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_08.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9876015-5faa-456a-937f-ca77a710fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_09.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d56178-1cab-4774-a3eb-22561f9a8b8e",
   "metadata": {},
   "source": [
    "We get a much more accurate model. We still observe some errors mainly due to\n",
    "the noise added to the dataset.\n",
    "\n",
    "## Gaussian process regression\n",
    "\n",
    "Now, we will use a\n",
    "`GaussianProcessRegressor` to fit the same\n",
    "dataset. When training a Gaussian process, the hyperparameters of the kernel\n",
    "are optimized during the fitting process. There is no need for an external\n",
    "hyperparameter search. Here, we create a slightly more complex kernel than\n",
    "for the kernel ridge regressor: we add a\n",
    "`WhiteKernel` that is used to\n",
    "estimate the noise in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a8ae32-af5d-4a93-bbec-790d421b3985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import WhiteKernel\n",
    "\n",
    "kernel = 1.0 * ExpSineSquared(1.0, 5.0, periodicity_bounds=(1e-2, 1e1)) + WhiteKernel(\n",
    "    1e-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18433e3-c30a-420d-a29b-c99b52cc8825",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    Using the kernel defined above,\n",
    "    <ul>\n",
    "        <li>Create a <tt>GaussianProcessRegressor</tt></li>\n",
    "        <li>Measure the time to fit the model.</li>\n",
    "        <li>Plot the predictions for the entire dataset.</li>\n",
    "    </ul>\n",
    "    Is the model properly fitting the data? What is the computational cost of the model?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e759e108-fcdd-463c-a874-6911c78d7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_10.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32781b02-966c-49b9-8e33-7cce796d3e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_11.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077d293d-25f4-470b-83a3-c653c25c7aca",
   "metadata": {},
   "source": [
    "The computation cost of training a Gaussian process is much less than the\n",
    "kernel ridge that uses a randomized search. We can check the parameters of\n",
    "the kernels that we computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e42aa-39f7-44ef-8608-a3484942294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_12.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db59def1-631a-4580-bf1d-f133dd01d59b",
   "metadata": {},
   "source": [
    "Indeed, we see that the parameters have been optimized. Looking at the\n",
    "`periodicity` parameter, we see that we found a period close to the\n",
    "theoretical value `2 \\pi`. We can have a look now at the predictions of\n",
    "our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3a4bad-5337-4fff-8835-f2ea8fc250b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_13.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16de94e3-cac0-4d4e-9ebd-4da29b1b0b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_14.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ca0573-a5d6-4ce9-8704-fb02d8122f73",
   "metadata": {},
   "source": [
    "We observe that the results of the kernel ridge and the Gaussian process\n",
    "regressor are close. However, the Gaussian process regressor also provide\n",
    "an uncertainty information that is not available with a kernel ridge.\n",
    "Due to the probabilistic formulation of the target functions, the\n",
    "Gaussian process can output the standard deviation (or the covariance)\n",
    "together with the mean predictions of the target functions.\n",
    "\n",
    "However, it comes at a cost: the time to compute the predictions is higher\n",
    "with a Gaussian process.\n",
    "\n",
    "## Final conclusion\n",
    "\n",
    "We can give a final word regarding the possibility of the two models to\n",
    "extrapolate. Indeed, we only provided the beginning of the signal as a\n",
    "training set. Using a periodic kernel forces our model to repeat the pattern\n",
    "found on the training set. Using this kernel information together with the\n",
    "capacity of the both models to extrapolate, we observe that the models will\n",
    "continue to predict the sine pattern.\n",
    "\n",
    "Gaussian process allows to combine kernels together. Thus, we could associate\n",
    "the exponential sine squared kernel together with a radial basis function\n",
    "kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd8c73-a5af-4677-b7e0-42834ae63da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "kernel = 1.0 * ExpSineSquared(1.0, 5.0, periodicity_bounds=(1e-2, 1e1)) * RBF(\n",
    "    length_scale=15, length_scale_bounds=\"fixed\"\n",
    ") + WhiteKernel(1e-1)\n",
    "gaussian_process = GaussianProcessRegressor(kernel=kernel)\n",
    "gaussian_process.fit(training_data, training_noisy_target)\n",
    "mean_predictions_gpr, std_predictions_gpr = gaussian_process.predict(\n",
    "    data,\n",
    "    return_std=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec0198-7e46-4e2a-a726-26404ae697a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data, target, label=\"True signal\", linewidth=2, linestyle=\"dashed\")\n",
    "plt.scatter(\n",
    "    training_data,\n",
    "    training_noisy_target,\n",
    "    color=\"black\",\n",
    "    label=\"Noisy measurements\",\n",
    ")\n",
    "# Plot the predictions of the kernel ridge\n",
    "plt.plot(\n",
    "    data,\n",
    "    predictions_kr,\n",
    "    label=\"Kernel ridge\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"dashdot\",\n",
    ")\n",
    "# Plot the predictions of the gaussian process regressor\n",
    "plt.plot(\n",
    "    data,\n",
    "    mean_predictions_gpr,\n",
    "    label=\"Gaussian process regressor\",\n",
    "    linewidth=2,\n",
    "    linestyle=\"dotted\",\n",
    ")\n",
    "plt.fill_between(\n",
    "    data.ravel(),\n",
    "    mean_predictions_gpr - std_predictions_gpr,\n",
    "    mean_predictions_gpr + std_predictions_gpr,\n",
    "    color=\"tab:green\",\n",
    "    alpha=0.2,\n",
    ")\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "_ = plt.title(\"Effect of using a RBF kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f96e8da-da0e-49e4-b606-d7f5f13e6cb8",
   "metadata": {},
   "source": [
    "The effect of using a radial basis function kernel will attenuate the\n",
    "periodicity effect once that no sample are available in the training.\n",
    "As testing samples get further away from the training ones, predictions\n",
    "are converging towards their mean and their standard deviation\n",
    "also increases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
